{"version":"NotebookV1","origId":1053375282066963,"name":"02a - Hyperopt","language":"python","commands":[{"version":"CommandV1","origId":1053375282066964,"guid":"b00c44cc-ce2d-44b6-93ab-5ab16d257ad2","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n# Hyperopt\n\nHyperopt is a Python library for \"serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions\".\n\nIn the machine learning workflow, hyperopt can be used to distribute/parallelize the hyperparameter optimization process with more advanced optimization strategies than are available in other libraries.\n\nThere are two ways to scale hyperopt with Apache Spark:\n* Use single-machine hyperopt with a distributed training algorithm (e.g. MLlib)\n* Use distributed hyperopt with single-machine training algorithms (e.g. scikit-learn) with the SparkTrials class. \n\nIn this lesson, we will use single-machine hyperopt with MLlib, but in the lab, you will see how to use hyperopt to distribute the hyperparameter tuning of single node models. \n\nUnfortunately you canâ€™t use hyperopt to distribute the hyperparameter optimization for distributed training algorithms at this time. However, you do still get the benefit of using more advanced hyperparameter search algorthims (random search, TPE, etc.) with Spark ML.\n\n\nResources:\n- [Documentation](http://hyperopt.github.io/hyperopt/scaleout/spark/)\n- [Hyperopt on Databricks](https://docs.databricks.com/applications/machine-learning/automl/hyperopt/index.html)\n- [Hyperparameter Tuning with MLflow, Apache Spark MLlib and Hyperopt](https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html)\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this notebook, we will:<br>\n - Use hyperopt to find the optimal parameters for an MLlib model using TPE","commandVersion":7,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"d042b71c-be6a-456b-aa6d-3bd290bcb74d"},{"version":"CommandV1","origId":1053375282066965,"guid":"991c2b4d-4937-439b-bbf2-2004a2db1d1a","subtype":"command","commandType":"auto","position":3.0,"command":"%run ./_setup","commandVersion":2,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397691164,"submitTime":1681397691164,"finishTime":1681397780117,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"42dbf264-9531-491e-a3ef-b0c14d6dbcdf"},{"version":"CommandV1","origId":1053375282066969,"guid":"ff055982-29b3-4b20-b105-b7df52016083","subtype":"command","commandType":"auto","position":4.0,"command":"%md\nLet's start by loading in our LendingClub dataset.","commandVersion":3,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"96c39a4b-bbf2-4b16-928a-8c3c1033263e"},{"version":"CommandV1","origId":1053375282066970,"guid":"62e5144c-a742-4872-be2a-229ad07023a5","subtype":"command","commandType":"auto","position":5.0,"command":"lending_club_df = spark.table(f\"{schema_name}.training_set\")\ntrain_df, test_df = lending_club_df.randomSplit([.8, .2], seed=42)","commandVersion":10,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"lending_club_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"term","nullable":true,"type":"string"},{"metadata":{},"name":"int_rate","nullable":true,"type":"double"},{"metadata":{},"name":"installment","nullable":true,"type":"double"},{"metadata":{},"name":"sub_grade","nullable":true,"type":"string"},{"metadata":{},"name":"home_ownership","nullable":true,"type":"string"},{"metadata":{},"name":"annual_inc","nullable":true,"type":"double"},{"metadata":{},"name":"verification_status","nullable":true,"type":"string"},{"metadata":{},"name":"loan_status","nullable":true,"type":"integer"},{"metadata":{},"name":"purpose","nullable":true,"type":"string"},{"metadata":{},"name":"dti","nullable":true,"type":"double"},{"metadata":{},"name":"earliest_cr_line","nullable":true,"type":"double"},{"metadata":{},"name":"open_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_bal","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_util","nullable":true,"type":"double"},{"metadata":{},"name":"total_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"initial_list_status","nullable":true,"type":"string"},{"metadata":{},"name":"application_type","nullable":true,"type":"string"},{"metadata":{},"name":"mort_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec_bankruptcies","nullable":true,"type":"double"},{"metadata":{},"name":"dti_na","nullable":true,"type":"double"},{"metadata":{},"name":"revol_util_na","nullable":true,"type":"double"},{"metadata":{},"name":"weight","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":"abfss://ucmetastore-cus@brnunitymetastorecus.dfs.core.windows.net/bofatest/ea8f1737-b033-46dc-a290-2f3e0e4321fc/tables/e9b1fef2-7766-4a7e-9ef2-37820fe78646"},{"name":"train_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"term","nullable":true,"type":"string"},{"metadata":{},"name":"int_rate","nullable":true,"type":"double"},{"metadata":{},"name":"installment","nullable":true,"type":"double"},{"metadata":{},"name":"sub_grade","nullable":true,"type":"string"},{"metadata":{},"name":"home_ownership","nullable":true,"type":"string"},{"metadata":{},"name":"annual_inc","nullable":true,"type":"double"},{"metadata":{},"name":"verification_status","nullable":true,"type":"string"},{"metadata":{},"name":"loan_status","nullable":true,"type":"integer"},{"metadata":{},"name":"purpose","nullable":true,"type":"string"},{"metadata":{},"name":"dti","nullable":true,"type":"double"},{"metadata":{},"name":"earliest_cr_line","nullable":true,"type":"double"},{"metadata":{},"name":"open_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_bal","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_util","nullable":true,"type":"double"},{"metadata":{},"name":"total_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"initial_list_status","nullable":true,"type":"string"},{"metadata":{},"name":"application_type","nullable":true,"type":"string"},{"metadata":{},"name":"mort_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec_bankruptcies","nullable":true,"type":"double"},{"metadata":{},"name":"dti_na","nullable":true,"type":"double"},{"metadata":{},"name":"revol_util_na","nullable":true,"type":"double"},{"metadata":{},"name":"weight","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"test_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"term","nullable":true,"type":"string"},{"metadata":{},"name":"int_rate","nullable":true,"type":"double"},{"metadata":{},"name":"installment","nullable":true,"type":"double"},{"metadata":{},"name":"sub_grade","nullable":true,"type":"string"},{"metadata":{},"name":"home_ownership","nullable":true,"type":"string"},{"metadata":{},"name":"annual_inc","nullable":true,"type":"double"},{"metadata":{},"name":"verification_status","nullable":true,"type":"string"},{"metadata":{},"name":"loan_status","nullable":true,"type":"integer"},{"metadata":{},"name":"purpose","nullable":true,"type":"string"},{"metadata":{},"name":"dti","nullable":true,"type":"double"},{"metadata":{},"name":"earliest_cr_line","nullable":true,"type":"double"},{"metadata":{},"name":"open_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_bal","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_util","nullable":true,"type":"double"},{"metadata":{},"name":"total_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"initial_list_status","nullable":true,"type":"string"},{"metadata":{},"name":"application_type","nullable":true,"type":"string"},{"metadata":{},"name":"mort_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec_bankruptcies","nullable":true,"type":"double"},{"metadata":{},"name":"dti_na","nullable":true,"type":"double"},{"metadata":{},"name":"revol_util_na","nullable":true,"type":"double"},{"metadata":{},"name":"weight","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397780095,"submitTime":1681397691279,"finishTime":1681397783567,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"ca57bfa4-dd6b-4a4a-8816-08c3c179b855"},{"version":"CommandV1","origId":1053375282066971,"guid":"38d863bd-0be3-460b-805b-25a343e905f6","subtype":"command","commandType":"auto","position":6.0,"command":"%md\nWe will then create our XGBoost pipeline and binary classification evaluator.","commandVersion":6,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"e735980a-6c16-4465-b753-17b230365f9f"},{"version":"CommandV1","origId":1053375282066972,"guid":"fb994c60-6817-4a64-b0c0-578e21980dea","subtype":"command","commandType":"auto","position":7.0,"command":"from pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nimport pyspark.sql.types as T\nfrom sparkdl.xgboost import XgboostClassifier\n\n# String indexing\ncategorical_cols = [x.name for x in train_df.schema.fields if x.dataType == T.StringType()]\nindex_cols = [x + \"_index\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_cols, handleInvalid=\"skip\")\n\n# Filter for just numeric columns (and exclude loan_status, our label)\nnumeric_cols = [x.name for x in train_df.schema.fields if x.dataType != T.StringType() and x.name != \"loan_status\" and x.name != \"weight\"]\n\nassembler_cols = index_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n\n# Define pipeline and evaluator\nxgb = XgboostClassifier(labelCol=\"loan_status\", weightCol=\"weight\", objective=\"binary:logistic\")\n\nstages = [string_indexer, vec_assembler, xgb]\npipeline = Pipeline(stages=stages)\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"loan_status\")","commandVersion":93,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"/databricks/python_shell/dbruntime/PostImportHook.py:184: FutureWarning: `sparkdl.xgboost` is deprecated and will be removed in a future Databricks Runtime release. Use `xgboost.spark` instead. See https://learn.microsoft.com/azure/databricks/machine-learning/train-model/xgboost-spark.html#xgboost-migration for migration.\n  hook(module)\nThe xgboost training will use single worker and set nthread=1 (equal to `spark.task.cpus` config), If you need to increase threads number used in training, you can set `nthread` param.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397783578,"submitTime":1681397691292,"finishTime":1681397786471,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",524]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"564c2972-0291-4c56-9b7e-ca7bde6240d9"},{"version":"CommandV1","origId":1053375282066973,"guid":"98f145d7-4665-44db-a723-feebfa9804cc","subtype":"command","commandType":"auto","position":8.0,"command":"%md\nNext, we get to the hyperopt-specific part of the workflow.\n\nFirst, we define our **objective function**. The objective function has two primary requirements:\n\n1. An **input** `params` including hyperparameter values to use when training the model\n2. An **output** containing a loss metric on which to optimize\n\nIn this case, we are specifying values of `n_estimators`, `learning_rate`, and so on, and returning AUC-ROC as our loss metric. Because we want to maximize this value in practice, we negate it in the loss function.\n\nWe are reconstructing our pipeline for the `XGBoostClassifier` to use the specified hyperparameter values.","commandVersion":27,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"78e2f9bd-5195-4fa1-b4bc-09e311d3bc6c"},{"version":"CommandV1","origId":1053375282066974,"guid":"8e2ca5da-50e8-4f0c-b062-f4a5dd639aa3","subtype":"command","commandType":"auto","position":9.0,"command":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\n\ndef objective_function(params):    \n  \n  # Set the hyperparameters that we want to tune\n  n_estimators = int(params[\"n_estimators\"])\n  eta = params[\"eta\"]\n  max_depth = int(params[\"max_depth\"])\n  min_child_weight = int(params[\"min_child_weight\"])\n  subsample = params[\"subsample\"]\n  gamma = params[\"gamma\"]\n  colsample_bytree = params[\"colsample_bytree\"]\n  \n  # Create a grid with our hyperparameters\n  grid = ParamGridBuilder() \\\n    .addGrid(xgb.n_estimators, [n_estimators]) \\\n    .addGrid(xgb.learning_rate, [eta]) \\\n    .addGrid(xgb.max_depth, [max_depth]) \\\n    .addGrid(xgb.min_child_weight, [min_child_weight]) \\\n    .addGrid(xgb.subsample, [subsample]) \\\n    .addGrid(xgb.gamma, [gamma]) \\\n    .addGrid(xgb.colsample_bytree, [colsample_bytree]) \\\n    .build()\n\n  # Cross validate the set of hyperparameters\n  cv = CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3)\n  cv_model = cv.fit(train_df)\n\n  # Get our average AUC across all three folds\n  auc_roc = cv_model.avgMetrics[0]\n\n  return {\"loss\": -auc_roc, \"status\": STATUS_OK}","commandVersion":84,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397786481,"submitTime":1681397691304,"finishTime":1681397786571,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"50b7c742-952a-46e6-83b6-f0528ec84327"},{"version":"CommandV1","origId":1053375282066975,"guid":"1cfaefd1-4ec1-4df2-95a6-a73ed61119ce","subtype":"command","commandType":"auto","position":10.0,"command":"%md\nNext, we define our search space. \n\nThis is similar to the parameter grid in a grid search process. However, we are only specifying the range of values rather than the individual, specific values to be tested. It's up to hyperopt's optimization algorithm to choose the actual values.\n\nSee the [documentation](https://github.com/hyperopt/hyperopt/wiki/FMin) for helpful tips on defining your search space.","commandVersion":0,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"9486cbd1-82e2-4106-b2e1-857aab9fd841"},{"version":"CommandV1","origId":1053375282066976,"guid":"10308c3d-735a-4f35-b5eb-f5879e9ff12b","subtype":"command","commandType":"auto","position":11.0,"command":"from hyperopt import hp\nimport numpy as np\n\nsearch_space = {\n  'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n  'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n  'max_depth': hp.quniform('max_depth', 1, 12, 1),\n  'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n  'subsample': hp.quniform('subsample', 0.2, 0.8, 0.05),\n  'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n  'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 0.8, 0.05)\n}","commandVersion":45,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397786580,"submitTime":1681397691319,"finishTime":1681397787374,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"920edd49-1cd1-451e-8130-83e79ca1937d"},{"version":"CommandV1","origId":1053375282066977,"guid":"7f6891ae-fb6a-4db3-8572-ba9693eee828","subtype":"command","commandType":"auto","position":12.0,"command":"%md\n\n`fmin()` generates new hyperparameter configurations to use for your `objective_function`. It will evaluate 4 models in total, using the information from the previous models to make a more informative decision for the the next hyperparameter to try. \n\nHyperopt allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). Note that in the cell below, we are importing `tpe`. According to the [documentation](http://hyperopt.github.io/hyperopt/scaleout/spark/), TPE is an adaptive algorithm that \n\n> iteratively explores the hyperparameter space. Each new hyperparameter setting tested will be chosen based on previous results. \n\nHence, `tpe.suggest` is a Bayesian method.\n\nMLflow also integrates with Hyperopt, so you can track the results of all the models youâ€™ve trained and their results as part of your hyperparameter tuning. Notice you can track the MLflow experiment in this notebook, but you can also specify an external experiment. ","commandVersion":0,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"59713a74-16e7-4c70-ab92-ffb85eb4ad32"},{"version":"CommandV1","origId":1053375282066978,"guid":"5d35e590-1021-4f57-9d6c-6cdd0f931b5e","subtype":"command","commandType":"auto","position":13.0,"command":"from hyperopt import fmin, tpe, STATUS_OK, Trials\n\n# Creating a parent run\nwith mlflow.start_run(run_name=f\"xgboost_{cleaned_username}\") as run:\n  num_evals = 10\n  trials = Trials()\n  best_hyperparam = fmin(\n    fn=objective_function,\n    space=search_space,\n    algo=tpe.suggest, \n    max_evals=num_evals,\n    trials=trials,\n    rstate=np.random.default_rng(42)\n  )\n  \n  # Get optimal hyperparameter values\n  best_n_estimators = int(best_hyperparam[\"n_estimators\"])\n  best_eta = best_hyperparam[\"eta\"]\n  best_max_depth = int(best_hyperparam[\"max_depth\"])\n  best_min_child_weight = int(best_hyperparam[\"min_child_weight\"])\n  best_subsample = best_hyperparam[\"subsample\"]\n  best_gamma = best_hyperparam[\"gamma\"]\n  best_colsample_bytree = best_hyperparam[\"colsample_bytree\"]\n  \n  # Change xgb to use optimal hyperparameter values (this is a stateful method)\n  xgb.set(xgb.n_estimators, best_n_estimators)\n  xgb.set(xgb.learning_rate, best_eta)\n  xgb.set(xgb.max_depth, best_max_depth)\n  xgb.set(xgb.min_child_weight, best_min_child_weight)\n  xgb.set(xgb.subsample, best_subsample)\n  xgb.set(xgb.gamma, best_gamma)\n  xgb.set(xgb.colsample_bytree, best_colsample_bytree)\n  \n  # Train pipeline on entire training data - this will use the updated xgb values\n  pipeline_model = pipeline.fit(train_df)\n  \n  # Evaluate final model on test data\n  pred_df = pipeline_model.transform(test_df)\n  multi_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"loan_status\")\n  \n  accuracy = multi_evaluator.evaluate(pred_df, {multi_evaluator.metricName: \"accuracy\"})\n  f1_score = multi_evaluator.evaluate(pred_df, {multi_evaluator.metricName: \"f1\"})\n  weighted_precision = multi_evaluator.evaluate(pred_df, {multi_evaluator.metricName: \"weightedPrecision\"})\n  weighted_recall = multi_evaluator.evaluate(pred_df, {multi_evaluator.metricName: \"weightedRecall\"})\n  auc_roc = evaluator.evaluate(pred_df)\n  \n  # Log params and metrics for the final model\n  mlflow.log_param(\"n_estimators\", best_n_estimators)\n  mlflow.log_param(\"eta\", best_eta)\n  mlflow.log_param(\"max_depth\", best_max_depth)\n  mlflow.log_param(\"min_child_weight\", best_min_child_weight)\n  mlflow.log_param(\"subsample\", best_subsample)\n  mlflow.log_param(\"gamma\", best_gamma)\n  mlflow.log_param(\"colsample_bytree\", best_colsample_bytree)\n  \n  mlflow.log_metric(\"accuracy\", accuracy)\n  mlflow.log_metric(\"f1_score\", f1_score)\n  mlflow.log_metric(\"weighted_precision\", weighted_precision)\n  mlflow.log_metric(\"weighted_recall\", weighted_recall)\n  mlflow.log_metric(\"auc_roc\", auc_roc)","commandVersion":109,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"\r  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 10%|â–ˆ         | 1/10 [01:24<12:36, 84.03s/trial, best loss: -0.5449524703172473]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 20%|â–ˆâ–ˆ        | 2/10 [01:35<05:32, 41.50s/trial, best loss: -0.5449524703172473]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:43<03:03, 26.22s/trial, best loss: -0.5449524703172473]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:51<01:54, 19.04s/trial, best loss: -0.546930491518703] ","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:59<01:14, 14.90s/trial, best loss: -0.546930491518703]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:06<00:49, 12.36s/trial, best loss: -0.554826169599458]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:18<00:35, 11.99s/trial, best loss: -0.554826169599458]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:26<00:21, 10.71s/trial, best loss: -0.554826169599458]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:39<00:11, 11.72s/trial, best loss: -0.554826169599458]","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\nWe recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 10.45s/trial, best loss: -0.5605394909658706]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.76s/trial, best loss: -0.5605394909658706]\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},{"type":"ansi","data":"We recommend using 0.0 as missing value to achieve better performance, but you set missing param to be nan. In the case of missing != 0, for features sparse vector input, the inactive values will be treated as 0 instead of missing values, and the active values which are nan will be treated as missing value, and this case the input sparse vector will be densified when constructing XGBoost DMatrix, if feature sparsity is high and input dataset is large, then it may slow down performance or lead to out of memory.\n","name":"stderr","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"pred_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"term","nullable":true,"type":"string"},{"metadata":{},"name":"int_rate","nullable":true,"type":"double"},{"metadata":{},"name":"installment","nullable":true,"type":"double"},{"metadata":{},"name":"sub_grade","nullable":true,"type":"string"},{"metadata":{},"name":"home_ownership","nullable":true,"type":"string"},{"metadata":{},"name":"annual_inc","nullable":true,"type":"double"},{"metadata":{},"name":"verification_status","nullable":true,"type":"string"},{"metadata":{},"name":"loan_status","nullable":true,"type":"integer"},{"metadata":{},"name":"purpose","nullable":true,"type":"string"},{"metadata":{},"name":"dti","nullable":true,"type":"double"},{"metadata":{},"name":"earliest_cr_line","nullable":true,"type":"double"},{"metadata":{},"name":"open_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_bal","nullable":true,"type":"integer"},{"metadata":{},"name":"revol_util","nullable":true,"type":"double"},{"metadata":{},"name":"total_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"initial_list_status","nullable":true,"type":"string"},{"metadata":{},"name":"application_type","nullable":true,"type":"string"},{"metadata":{},"name":"mort_acc","nullable":true,"type":"integer"},{"metadata":{},"name":"pub_rec_bankruptcies","nullable":true,"type":"double"},{"metadata":{},"name":"dti_na","nullable":true,"type":"double"},{"metadata":{},"name":"revol_util_na","nullable":true,"type":"double"},{"metadata":{},"name":"weight","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"name":"term_index","type":"nominal","vals":[" 36 months"," 60 months"]}},"name":"term_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"sub_grade_index","type":"nominal","vals":["B2","B4","B5","C1","B1","C2","A4","C3","C4","A3","A5","B3","C5","D2","A2","D1","D5","D3","D4","A1","E5","E4","E3","E1","E2","F1","G1","F3","F4","F5","F2"]}},"name":"sub_grade_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"home_ownership_index","type":"nominal","vals":["MORTGAGE","RENT","OWN"]}},"name":"home_ownership_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"verification_status_index","type":"nominal","vals":["Source Verified","Not Verified","Verified"]}},"name":"verification_status_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"purpose_index","type":"nominal","vals":["debt_consolidation","credit_card","other","home_improvement","major_purchase","house","medical","car","moving","small_business","vacation","renewable_energy"]}},"name":"purpose_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"initial_list_status_index","type":"nominal","vals":["w","f"]}},"name":"initial_list_status_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"name":"application_type_index","type":"nominal","vals":["Individual","Joint App"]}},"name":"application_type_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"loan_amnt"},{"idx":1,"name":"int_rate"},{"idx":2,"name":"installment"},{"idx":3,"name":"annual_inc"},{"idx":4,"name":"dti"},{"idx":5,"name":"earliest_cr_line"},{"idx":6,"name":"open_acc"},{"idx":7,"name":"pub_rec"},{"idx":8,"name":"revol_bal"},{"idx":9,"name":"revol_util"},{"idx":10,"name":"total_acc"},{"idx":11,"name":"mort_acc"},{"idx":12,"name":"pub_rec_bankruptcies"},{"idx":13,"name":"dti_na"},{"idx":14,"name":"revol_util_na"}]},"num_attrs":15}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"rawPrediction","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"prediction","nullable":true,"type":"double"},{"metadata":{},"name":"probability","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397787391,"submitTime":1681397691330,"finishTime":1681397963662,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",55],["ansi",2064],["ansi",82],["ansi",2064],["ansi",82],["ansi",2064],["ansi",82],["ansi",2064],["ansi",82],["ansi",2064],["ansi",81],["ansi",2064],["ansi",81],["ansi",2064],["ansi",81],["ansi",2064],["ansi",81],["ansi",2064],["ansi",81],["ansi",2064],["ansi",167],["ansi",516]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"e5e501b0-3531-490c-9db6-41baec919110"},{"version":"CommandV1","origId":1053375282066979,"guid":"34706443-402d-4c9f-9727-08db908921ab","subtype":"command","commandType":"auto","position":14.0,"command":"%md-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>","commandVersion":0,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"4dca7ca9-8d82-4d3c-8deb-ad9bbddfb9a4"},{"version":"CommandV1","origId":3017262774490327,"guid":"095c7d41-b177-47bd-aca5-5a2b9d092193","subtype":"script","commandType":"auto","position":3.2,"command":"","commandVersion":2,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397722181,"submitTime":0,"finishTime":1681397722730,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["991c2b4d-4937-439b-bbf2-2004a2db1d1a"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"bde0d089-4f0c-452f-9104-62714fa5a5a5"},{"version":"CommandV1","origId":3017262774490328,"guid":"6c0cd004-dc81-4993-9b65-7391e6076d47","subtype":"script","commandType":"auto","position":3.4,"command":"","commandVersion":19,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"Out[2]: True","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397722802,"submitTime":0,"finishTime":1681397723436,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["991c2b4d-4937-439b-bbf2-2004a2db1d1a"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",12]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"47078a36-5a6a-456d-8e0c-d6383b94aa54"},{"version":"CommandV1","origId":3017262774490329,"guid":"426396db-473b-46c0-9e81-a06c7ef1ceac","subtype":"script","commandType":"auto","position":3.6,"command":"","commandVersion":15,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397723470,"submitTime":0,"finishTime":1681397723636,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["991c2b4d-4937-439b-bbf2-2004a2db1d1a"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"6f2bd1f1-07da-458b-b606-4abf51e24716"},{"version":"CommandV1","origId":3017262774490330,"guid":"5fb29c4c-c485-46b5-b52a-0db213a39cef","subtype":"script","commandType":"auto","position":3.8,"command":"","commandVersion":15,"state":"finished","results":{"type":"listResults","data":[{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[],"columnCustomDisplayInfos":{},"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1681397723647,"submitTime":0,"finishTime":1681397780061,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["991c2b4d-4937-439b-bbf2-2004a2db1d1a"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"Change the catalog here to one the users will have create schema permissions in","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["table",0]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"9fb35144-706d-4939-b438-262647d70754"}],"dashboards":[],"guid":"a9d8e8eb-5084-46de-896c-1e64f6a4007a","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1053375282066965,"dataframes":["_sqldf"]}},"reposExportFormat":"SOURCE"}